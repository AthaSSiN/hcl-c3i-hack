import tensorflow as tf
from tensorflow import keras

from keras import models, regularizers
from keras.regularizers import l2
import numpy as np
import glob
import os
import json
from sklearn.feature_extraction import FeatureHasher
import csv
import tqdm
import re
import time

class parse_Structure():
    """
    {"gen_info":{},"header_info":{},"imported_funcs":{},"section_info":{}}
    """
    def __init__(self,file,target={"gen_info":{},"header_info":{},"imported_funcs":{},"section_info":[]}):
        #self.file=open(file,'r',encoding='latin-1').read()
        self.file=file # for directly stored copy on ram
        self.target=target
    def gen_info(self):
        # number of dlls
        self.target['gen_info']['ImportedFunctions']=len([f for f in re.findall('IMAGE_IMPORT_DESCRIPTOR',self.file) if f!=''])
        # presence of debug session. 1 or 0.
        t=[f for f in re.findall("----------Debug information----------",self.file) if f!='']
        self.target['gen_info']['HasDebug']=1 if len(t) is 1 else 0
        # presence of resources
        t=[f for f in re.findall("----------Resource directory----------",self.file) if f!='']
        self.target['gen_info']['HasResources']=1 if len(t) is 1 else 0
        # Presence of relocations
        t=[f for f in re.findall("IMAGE_FILE_RELOCS_STRIPPED",self.file) if f!='']
        self.target['gen_info']['HasRelocation']=1 if len(t) is 0 else 0
        # number of symbols
        t=[f for f in re.findall("(NumberOfSymbols:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['gen_info']['NumSymbols']=int(re.findall('0x[^ ]*',t[0])[0],0)
        
    def header_info(self):
        # timestamp
        t=[f for f in re.findall("(TimeDateStamp:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['TimeDateStamp']=int(re.findall('0x[^ ]*',t[0])[0],0)
        # list of characteristics
        t=[f for f in re.findall("(Flags:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['ListOfCharacteristics']=t[0][7:].split(', ')
        # Magic number
        t=[f for f in re.findall("(Magic:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['MagicNumber']="PE32" if int(re.findall('0x[^ ]*',t[0])[0],0) is 267 else "PE32+"
        # major image version
        t=[f for f in re.findall("(MajorImageVersion:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['MajorImageVersion']=int(re.findall('0x[^ ]*',t[0])[0],0)
        # Minor image version
        t=[f for f in re.findall("(MinorImageVersion:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['MinorImageVersion']=int(re.findall('0x[^ ]*',t[0])[0],0)
        # major linker version
        t=[f for f in re.findall("(MajorLinkerVersion:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['MajorLinkerVersion']=int(re.findall('0x[^ ]*',t[0])[0],0)
        # minor linker version
        t=[f for f in re.findall("(MinorLinkerVersion:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['MinorLinkerVersion']=int(re.findall('0x[^ ]*',t[0])[0],0)
        # major os version
        t=[f for f in re.findall("(MajorOperatingSystemVersion:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['MajorOperatingSystemVersion']=int(re.findall('0x[^ ]*',t[0])[0],0)
        # minor os version
        t=[f for f in re.findall("(MinorOperatingSystemVersion:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['MinorOperatingSystemVersion']=int(re.findall('0x[^ ]*',t[0])[0],0)
        # code size
        t=[f for f in re.findall("(SizeOfCode:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['SizeOfCode']=int(re.findall('0x[^ ]*',t[0])[0],0)
        # header size
        t=[f for f in re.findall("(SizeOfHeaders:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['SizeOfHeaders']=int(re.findall('0x[^ ]*',t[0])[0],0)
        # commit size
        t=[f for f in re.findall("(SizeOfHeapCommit:[ ]*.*[^\s]+)*",self.file) if f!='']
        self.target['header_info']['SizeOfCommit']=int(re.findall('0x[^ ]*',t[0])[0],0)
        
    def imported_funcs(self):
        t=[f for f in re.findall(".*[.]dll[.][^ ]*",self.file) if f!='']
        funcdict={}
        for dll in t:
            dll=dll.split('.dll.')
            if dll[0]+'.dll' not in funcdict:
                funcdict[dll[0]+'.dll']=[]
            funcdict[dll[0]+'.dll'].append(dll[1])
        self.target['imported_funcs']=funcdict
        
    def section_info(self):
        num_files=len([f for f in re.findall("\[IMAGE_SECTION_HEADER\]\n0x",self.file) if f!=''])
        self.target['section_info']=[{} for _ in range(num_files)] # append empty dicts in list
        # section name
        sec_name=[f for f in re.findall("(Name:[ ]*.*[^\s]+)*",self.file) if f!='']
        sec_name=sec_name[:num_files]
        # virtual size
        vsize=[f for f in re.findall("(Misc_VirtualSize:[ ]*.*[^\s]+)*",self.file) if f!='']
        # size
        section_size=[f for f in re.findall("(SizeOfRawData:[ ]*.*[^\s]+)*",self.file) if f!='']
        # entropy
        entropy=[f for f in re.findall("(Entropy: [0-7].[0-9]*)*",self.file) if f!='']
        # section characteristics
        secchar=[f[7:-8] for f in re.findall("Flags:.*\nEntropy",self.file) if f!='']
        for i,dc in enumerate(self.target['section_info']):
            # dc is dict
            dc['name']=sec_name[i].split(' ')[-1]
            dc['vsize']=int(re.findall('0x[^ ]*',vsize[i])[0],0)
            dc['size']=int(re.findall('0x[^ ]*',section_size[i])[0],0)
            dc['entropy']=float(entropy[i].split(' ')[-1])
            dc['SectionCharacteristics']=secchar[i].split(', ')
#             dc['EntryPoint']=int(re.findall('0x[^ ]*',entryp[0])[0],0)
    
    def get_dict(self):
        self.gen_info()
        self.header_info()
        self.imported_funcs()
        self.section_info()
        return self.target
    
    def get_vector(self):
        req_dict=self.get_dict()
        gen_vec=np.array([
            req_dict['gen_info']['ImportedFunctions'],
            req_dict['gen_info']['HasDebug'],
            req_dict['gen_info']['HasResources'],
            req_dict['gen_info']['HasRelocation'],
            req_dict['gen_info']['NumSymbols']
        ])
        header_vec=np.hstack([
            req_dict['header_info']['TimeDateStamp'],
            FeatureHasher(10, input_type="string").transform([req_dict['header_info']['ListOfCharacteristics']]).toarray()[0],
            FeatureHasher(10, input_type="string").transform([req_dict['header_info']['MagicNumber']]).toarray()[0],
            req_dict['header_info']['MajorImageVersion'],
            req_dict['header_info']['MinorImageVersion'],
            req_dict['header_info']['MajorLinkerVersion'],
            req_dict['header_info']['MinorLinkerVersion'],
            req_dict['header_info']['MajorOperatingSystemVersion'],
            req_dict['header_info']['MinorOperatingSystemVersion'],
            req_dict['header_info']['SizeOfCode'],
            req_dict['header_info']['SizeOfHeaders'],
            req_dict['header_info']['SizeOfCommit']
        ])
        libraries=[lib for lib in req_dict['imported_funcs']]
        imports=[]
        for lib in libraries:
            for func in req_dict['imported_funcs'][lib]:
                imports.append(lib+'.'+func)
        imp_funcs=np.hstack([
            FeatureHasher(256, input_type="string").transform([libraries]).toarray()[0],
            FeatureHasher(1024, input_type="string").transform([imports]).toarray()[0],
        ])
        
        section_sizes = [(s['name'], s['size']) for s in req_dict['section_info']]
        section_sizes_hashed = FeatureHasher(50, input_type="pair").transform([section_sizes]).toarray()[0]
        section_entropy = [(s['name'], s['entropy']) for s in req_dict['section_info']]
        section_entropy_hashed = FeatureHasher(50, input_type="pair").transform([section_entropy]).toarray()[0]
        section_vsize = [(s['name'], s['vsize']) for s in req_dict['section_info']]
        section_vsize_hashed = FeatureHasher(50, input_type="pair").transform([section_vsize]).toarray()[0]
        characteristics = []
        for c in req_dict['section_info']:
            characteristics.append(c['SectionCharacteristics'])
        characteristics_hashed = FeatureHasher(50, input_type="string").transform(characteristics).toarray()[0],
#         print(type(characteristics_hashed[0]))
        sec_info=np.hstack([
            section_sizes_hashed,
            section_entropy_hashed,
            section_vsize_hashed,
            characteristics_hashed[0]
        ])

        return np.hstack([
            gen_vec,
            header_vec,
            imp_funcs,
            sec_info
        ])

class Static():

  def __init__(self):
    self.model = keras.Sequential()
    self.model.add(keras.layers.Dense(512, activation='relu', kernel_initializer = 'he_uniform' , kernel_regularizer = l2(1e-4), input_shape = (1515,)))
    self.model.add(keras.layers.Dense(256, activation='relu', kernel_initializer = 'he_uniform' , kernel_regularizer = l2(1e-4)))
    self.model.add(keras.layers.Dense(128, activation='relu', kernel_initializer = 'he_uniform' , kernel_regularizer = l2(1e-4)))
    self.model.add(keras.layers.Dense(128, activation='relu', kernel_initializer = 'he_uniform' , kernel_regularizer = l2(1e-4)))
    self.model.add(keras.layers.Dense(1, kernel_initializer = 'he_uniform'))

    self.model.compile(optimizer= 'adam', 
              loss=tf.losses.BinaryCrossentropy(from_logits = True),
              metrics=['accuracy'])
    
  def dataloader(self):
    structure_info=[f for f in glob.iglob('**/Structure_Info.txt',recursive=True)]
    structures=[]
    for i in range(len(structure_info)):
      f=open(structure_info[i],'r',encoding='latin-1').read()
      if f is not '':
        structures.append(f)
    vectors=np.zeros((1515,1)) # -> list of all vectors created by parse_Structure
    labels=np.zeros((1,1)) # list of labels of vectors
    for i in tqdm.tqdm(range(len(structures))):
      is_mal=np.array([1.]) if 'Malware' in structure_info[i] else np.array([0.])
      target=parse_Structure(file=structures[i])
      target=np.reshape(target.get_vector(),(-1,1))
      is_mal=np.reshape(is_mal,(-1,1))
      vectors=np.concatenate((vectors,target), axis=1)
      labels=np.concatenate((labels,is_mal), axis=1)
    np.save('vectors.npy',vectors)
    np.save('labels.npy',labels)

  def train(self):
    if not os.path.exists('vectors.npy'):
      self.dataloader()

    x = np.load('vectors.npy')
    y = np.load('labels.npy')

    x = np.transpose(x)
    y = y.reshape((y.shape[1], ))
    x = (x-np.mean(x))/np.std(x)
    for i in range(len(x)):
      for j in range(len(x[i])):
        x[i][j] = np.abs(x[i][j])

    self.model.fit(
      x, y,
      epochs=50
    )

    self.model.save_weights('wts_stat.h5', save_format = 'h5')

    # stores all text files in the memory
  def test(self, pth):
    self.model.load_weights('wts_stat.h5')
    structure_info=[f for f in glob.iglob('**/Structure_Info.txt',recursive=True)]
    structures=[]
    names = []
    preds = {}
    for i in range(len(structure_info)):
        f=open(structure_info[i],'r',encoding='latin-1').read()
        names.append(structure_info[i].split('/')[-2])
        if f is not '':
            structures.append(f)
    vectors=np.zeros((1515,1)) # -> list of all vectors created by parse_Structure
    
    for i in tqdm.tqdm(range(len(structures))):
        #try:
        target=parse_Structure(file=structures[i])
        target=np.reshape(target.get_vector(),(-1,1))
        vectors=np.concatenate((vectors,target), axis=1)

        #except:
            #print(i)
    np.save('vectors_test.npy',vectors)
    x = np.load('vectors_test.npy')

    x = np.transpose(x)
    x = (x-np.mean(x))/np.std(x)
    for i in range(len(x)):
      for j in range(len(x[i])):
        x[i][j] = np.abs(x[i][j])

    for i in range(0,len(x),32):
      ret = self.model.predict(x[i:min(i+32,len(x)),:])
      for k in range(len(ret)):
        preds[names[i+k]] = 'Malware' if ret[k] >= 0 else 'Benign'

    return preds

class Dynamic():

  def __init__(self):
    self.model = keras.Sequential()
    self.model.add(keras.layers.Dense(128, activation='relu', kernel_initializer = 'he_uniform' , kernel_regularizer = l2(1e-4), input_shape = (50,)))
    self.model.add(keras.layers.Dense(128, activation='relu', kernel_initializer = 'he_uniform' , kernel_regularizer = l2(1e-4)))
    self.model.add(keras.layers.Dense(128, activation='relu', kernel_initializer = 'he_uniform' , kernel_regularizer = l2(1e-4)))
    self.model.add(keras.layers.Dense(128, activation='relu', kernel_initializer = 'he_uniform' , kernel_regularizer = l2(1e-4)))
    self.model.add(keras.layers.Dense(1, kernel_initializer = 'he_uniform'))

    self.model.compile(optimizer= 'adam', 
              loss=tf.losses.BinaryCrossentropy(from_logits = True),
              metrics=['accuracy'])
    
  def dataloader(self):
    y = []
    x = []

    types = ['Benign', 'Malware/Backdoor','Malware/Trojan','Malware/TrojanDownloader','Malware/TrojanDropper','Malware/Virus','Malware/Worm']
    for t in types:
      path = "./**/" + t + "/*.json"
      files = glob.glob(path, recursive=True)
      total = len(files)

      for i,fil in tqdm.tqdm(enumerate(files)): 
        with open(fil) as f:
          f = json.load(f)
          arr = {}
          for obj in f["signatures"]:
            if obj["description"] + "_markcount" not in arr.keys():
              if 'AntiVirus' in obj["description"]:
                arr['antivirus_markcount'] = obj["markcount"]
                arr['antivirus_severity'] = obj["severity"]
              else:
                arr[obj["description"] + '_markcount'] = obj["markcount"]
                arr[obj["description"] + '_severity'] = obj["severity"]
          ans = FeatureHasher(50).transform([arr]).toarray()[0]
          
          x.append(ans)
          if t == 'Benign':
            y.append(0)
          else:
            y.append(1)
          if i%100 == 0:
            print(i)
            print(ans)
            print(len(x))

    x= np.array(x, dtype=np.int32)
    y = np.array(y, dtype=np.int32)

    np.savetxt('./x_train_dy.txt', x)
    np.savetxt('./y_train_dy.txt', y)
    return x,y
  
  def train(self):
    if not os.path.exists('./x_train_dy.txt'):
      x_train,y_train = self.dataloader()
    else:
      x_train = np.array(np.loadtxt('x_train_dy.txt'), dtype = np.int32)
      y_train = np.array(np.loadtxt('y_train_dy.txt'), dtype = np.int32)

    for i in range(len(x_train)):
      for j in range(len(x_train[i])):
        x_train[i][j] = np.abs(x_train[i][j])

    self.model.fit(
      x_train, y_train ,
      epochs=12
    )

    self.model.save_weights('wts_dy.h5', save_format = 'h5')

  def test(self, pt):
    self.model.load_weights('wts_dy.h5')
    x_test = []
    path =  pt + "/**/*.json"
    files = glob.glob(path, recursive=True)
    total = len(files)
    names = []
    preds = {}
    for i,fil in tqdm.tqdm(enumerate(files)): 
      with open(fil) as f:
        fili = fil.split('/')
        names.append(fili[-1][:-5])
        f = json.load(f)
        arr = {}
        for obj in f["signatures"]:
          if obj["description"] + "_markcount" not in arr.keys():
            if 'AntiVirus' in obj["description"]:
              arr['antivirus_markcount'] = obj["markcount"]
              arr['antivirus_severity'] = obj["severity"]
            else:
              arr[obj["description"] + '_markcount'] = obj["markcount"]
              arr[obj["description"] + '_severity'] = obj["severity"]
        ans = FeatureHasher(50).transform([arr]).toarray()[0]
        
        x_test.append(ans)

    x_test = np.array(x_test)
    for i in range(len(x_test)):
      for j in range(len(x_test[i])):
        x_test[i][j] = np.abs(x_test[i][j])

    for i in range(0,len(x_test),32):
      ret = self.model.predict(x_test[i:min(i+32,len(x_test)),:])
      for k in range(len(ret)):
        preds[names[i+k]] = 'Malware' if ret[k] >= 0 else 'Benign'

    return preds

stat = Static()

dict_stat = {}
dic_dyn = {}

path = input('Enter path to folder ')
if not os.path.exists('./wts_stat.h5'):
  stat.train()
  dict_stat = stat.test(path)
else:
  dict_stat = stat.test(path)

dyn = Dynamic()
if not os.path.exists('./wts_dy.h5'):
  dyn.train()
  dict_dyn = dyn.test(path)
else:
  dict_dyn = dyn.test(path)

with open('output.csv', 'w') as csv_file:  
  writer = csv.writer(csv_file)
  writer.writerow(["Hash", "Predictions"])
  for key, value in dict_stat.items():
    writer.writerow([key, value])
  for key, value in dict_dyn.items():
    writer.writerow([key, value])
